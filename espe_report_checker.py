import re
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any
import PyPDF2
from docx import Document
from dataclasses import dataclass, asdict
import logging

try:
    from langdetect import detect
    LANGUAGE_DETECTION_AVAILABLE = True
except ImportError:
    LANGUAGE_DETECTION_AVAILABLE = False
    print("‚ö†Ô∏è Dil tespiti i√ßin: pip install langdetect")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ESPECriteria:
    """ESPE rapor kriterleri veri sƒ±nƒ±fƒ±"""
    genel_rapor_bilgileri: Dict[str, Any]
    koruma_cihazi_bilgileri: Dict[str, Any]
    makine_durus_performansi: Dict[str, Any]
    guvenlik_mesafesi_hesabi: Dict[str, Any]
    gorsel_teknik_dokumantasyon: Dict[str, Any]
    sonuc_oneriler: Dict[str, Any]

@dataclass
class ESPEAnalysisResult:
    """ESPE analiz sonucu veri sƒ±nƒ±fƒ±"""
    criteria_name: str
    found: bool
    content: str
    score: int
    max_score: int
    details: Dict[str, Any]

class ESPEReportAnalyzer:
    """ESPE rapor analiz sƒ±nƒ±fƒ±"""
    
    def __init__(self):
        self.criteria_weights = {
            "Genel Rapor Bilgileri": 10,
            "Koruma Cihazƒ± (ESPE) Bilgileri": 10,
            "Makine Duru≈ü Performansƒ± √ñl√ß√ºm√º": 25,
            "G√ºvenlik Mesafesi Hesabƒ±": 25,
            "G√∂rsel ve Teknik D√∂k√ºmantasyon": 5,
            "Sonu√ß ve √ñneriler": 10
        }
        
        # √áoklu dil pattern'leri - EN ISO 13855 standardƒ±na g√∂re
        self.criteria_details = {
            "Genel Rapor Bilgileri": {
                "proje_adi_numarasi": {"pattern": r"(?:Proje\s*(?:Ad[ƒ±i]|No|Numaras[ƒ±i])[:]*\s*([A-Z]?\d+(?:\.\d+)?)|Project\s*(?:Name|No|Number)[:]*\s*([A-Z]?\d+)|C\d{2}\.\d{3})", "weight": 2},
                "olcum_tarihi": {"pattern": r"(?:√ñl√ß√ºm\s*Tarihi|Measurement\s*Date|Messdatum|\d{1,2}[./]\d{1,2}[./]\d{4})", "weight": 2},
                "rapor_tarihi": {"pattern": r"(?:Rapor\s*Tarihi|Report\s*Date|Berichtsdatum|\d{1,2}[./]\d{1,2}[./]\d{4})", "weight": 1},
                "makine_adi": {"pattern": r"(?:Makine\s*Ad[ƒ±i][:]*\s*(T\d+\s*-\s*MCC\d+|T\d+|MCC\d+)|Machine\s*Name[:]*\s*(T\d+\s*-\s*MCC\d+))", "weight": 2},
                "hat_bolge": {"pattern": r"(?:Hat|Line|Linie|B√∂lge|Area|Bereich|Zone|Jaws|\d+\.?\s*Hat)", "weight": 1},
                "olcum_yapan": {"pattern": r"(?:Hazƒ±rlayan|√ñl√ß√ºm√º\s*Yapan|Prepared\s*by|Measured\s*by|Erstellt\s*von|Gemessen\s*von|Pilz|Firma|Company)", "weight": 1},
                "imza_onay": {"pattern": r"(?:ƒ∞mza|Signature|Onay|Approval|ƒ∞nceleyen|Reviewed)", "weight": 1}
            },
            "Koruma Cihazƒ± (ESPE) Bilgileri": {
                "cihaz_tipi": {"pattern": r"(?:I≈üƒ±k\s*Perdesi|Light\s*Curtain|Lichtvorhang|ESPE|Safety\s*Device|Alan\s*Tarayƒ±cƒ±)", "weight": 3},
                "kategori": {"pattern": r"(?:Kategori|Category|Kategorie|Cat\s*[234])", "weight": 2},
                "koruma_yuksekligi": {"pattern": r"(?:Koruma\s*Y√ºksekliƒüi|Protection\s*Height|Schutzh√∂he|\d{3,4}\s*mm)", "weight": 3},
                "cozunurluk": {"pattern": r"(?:√á√∂z√ºn√ºrl√ºk|Resolution|Aufl√∂sung|d\s*deƒüeri|\d{1,2}\s*mm)", "weight": 2}
            },
            "Makine Duru≈ü Performansƒ± √ñl√ß√ºm√º": {
                "olcum_metodu": {"pattern": r"(?:√ñl√ß√ºm\s*Metodu|Test\s*Prosed√ºr√º|Measurement\s*Method|Test\s*Procedure|ESPE\s*√ñl√ß√ºm|Yapƒ±lan.*?√ñl√ß√ºm)", "weight": 4},
                "test_sayisi": {"pattern": r"(?:Test\s*Sayƒ±sƒ±|Tekrarlanabilirlik|Repeatability|Test\s*Count|\d+\s*test|\d+\s*√∂l√ß√ºm)", "weight": 4},
                "durus_suresi_min": {"pattern": r"Min\s*(\d{2,3})|Minimum\s*(\d{2,3})|En\s*Az\s*(\d{2,3})", "weight": 6},
                "durus_suresi_max": {"pattern": r"Maks?\.?\s*(\d{2,3})|Max\.?\s*(\d{2,3})|Maximum\s*(\d{2,3})|En\s*Fazla\s*(\d{2,3})", "weight": 6},
                "durus_mesafesi": {"pattern": r"(?:Duru≈ü\s*Mesafesi|Durma\s*Mesafesi|Stopping\s*Distance|Anhalteweg|STD|\d{2,4}\s*mm)", "weight": 5}
            },
            "G√ºvenlik Mesafesi Hesabƒ±": {
                "formula_s": {"pattern": r"S\s*=\s*\([^)]*[KT][^)]*\)", "weight": 8},
                "k_sabiti": {"pattern": r"(?:K\s*=\s*(\d{4})|2000\s*mm/s|1600\s*mm/s)", "weight": 5},
                "c_sabiti": {"pattern": r"C\s*=\s*8\s*[√óx*]\s*\(\s*d\s*[-‚Äì]\s*14\s*\)", "weight": 4},
                "t_durus_suresi": {"pattern": r"(?:T\s*[:=]|Duru≈ü\s*S√ºresi|Stopping\s*Time)", "weight": 4},
                "uygunluk_kontrolu": {"pattern": r"(?:Mevcut\s*mesafe|‚â•|>=|UYGUN|SUITABLE|UYGUNSUZ|UNSUITABLE)", "weight": 2},
                "alternatif_hesap": {"pattern": r"(?:500\s*mm|K\s*=\s*1600)", "weight": 2}
            },
            "G√∂rsel ve Teknik D√∂k√ºmantasyon": {
                "makine_espe_fotograf": {"pattern": r"(?:G√∂rsel|Fotoƒüraf|Resim|Photo|Image|Picture|Bild|Foto)", "weight": 3},
                "mesafe_olcumu_gorseli": {"pattern": r"(?:Mesafe|Distance|√ñl√ß√ºm|Measurement).*?(?:G√∂rsel|i≈üaretli|Marked)", "weight": 2}
            },
            "Sonu√ß ve √ñneriler": {
                "tehlike_tanimi": {"pattern": r"(?:Tehlikeli?\s*Hareket|Dangerous\s*Movement|Gef√§hrliche\s*Bewegung|Tehlike|fikst√ºr|pres|kapƒ±|hareket)", "weight": 3},
                "uygunluk_degerlendirme": {"pattern": r"(?:Uygun|Suitable|Geeignet|Uygunsuz|Unsuitable|Ungeeignet)", "weight": 2},
                "iyilestirme_onerileri": {"pattern": r"(?:√ñneri|Recommendation|Empfehlung|ƒ∞yile≈ütir|Improve|Verbessern|mesafe\s*arttƒ±r)", "weight": 3},
                "en_iso_baglanti": {"pattern": r"EN\s*ISO\s*13855", "weight": 2}
            }
        }
    
    def detect_language(self, text: str) -> str:
        """Metnin dilini tespit et"""
        if not LANGUAGE_DETECTION_AVAILABLE:
            return "unknown"
        
        try:
            # Kƒ±sa metin √∂rnekleri al
            sample_text = " ".join(text.split()[:100])
            detected_lang = detect(sample_text)
            
            # Ana diller
            if detected_lang in ['tr', 'turkish']:
                return 'turkish'
            elif detected_lang in ['en', 'english']:
                return 'english'
            elif detected_lang in ['de', 'german']:
                return 'german'
            else:
                return detected_lang
                
        except Exception as e:
            logger.warning(f"Dil tespiti hatasƒ±: {e}")
            return "unknown"

    def get_multilingual_patterns(self, criterion: str, detected_lang: str) -> List[str]:
        """Tespit edilen dile g√∂re ek pattern'ler d√∂nd√ºr"""
        additional_patterns = {
            'turkish': {
                'proje_adi_numarasi': [r"Proje\s*No", r"Belge\s*No", r"D√∂k√ºman"],
                'makine_adi': [r"Makine\s*Adƒ±", r"Ekipman", r"Cihaz"],
                'olcum_tarihi': [r"√ñl√ß√ºm\s*Tarihi", r"Test\s*Tarihi"],
                'tehlike_tanimi': [r"Tehlikeli", r"Risk", r"Tehlike"]
            },
            'english': {
                'proje_adi_numarasi': [r"Project\s*No", r"Document\s*No", r"Report\s*No"],
                'makine_adi': [r"Machine\s*Name", r"Equipment", r"Device"],
                'olcum_tarihi': [r"Measurement\s*Date", r"Test\s*Date"],
                'tehlike_tanimi': [r"Dangerous", r"Hazardous", r"Risk"]
            },
            'german': {
                'proje_adi_numarasi': [r"Projekt\s*Nr", r"Dokument\s*Nr", r"Bericht\s*Nr"],
                'makine_adi': [r"Maschinen\s*Name", r"Ausr√ºstung", r"Ger√§t"],
                'olcum_tarihi': [r"Messdatum", r"Testdatum"],
                'tehlike_tanimi': [r"Gef√§hrlich", r"Risiko", r"Gefahr"]
            }
        }
        
        return additional_patterns.get(detected_lang, {}).get(criterion, [])

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDF'den metin √ßƒ±karma"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            logger.error(f"PDF okuma hatasƒ±: {e}")
            return ""
    
    def extract_text_from_docx(self, docx_path: str) -> str:
        """DOCX'den metin √ßƒ±karma"""
        try:
            doc = Document(docx_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            logger.error(f"DOCX okuma hatasƒ±: {e}")
            return ""
    
    def check_report_date_validity(self, text: str) -> Tuple[bool, str, str]:
        """Rapor tarihinin ge√ßerliliƒüini kontrol etme"""
        date_patterns = [
            r"√ñl√ß√ºm\s*Tarihi\s*[:=]\s*(\d{2}[./]\d{2}[./]\d{4})",
            r"(\d{2}[./]\d{2}[./]\d{4})"
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            if matches:
                date_str = matches[0]
                try:
                    # Tarih formatƒ±nƒ± normalize et
                    date_str = date_str.replace('.', '/').replace('-', '/')
                    report_date = datetime.strptime(date_str, '%d/%m/%Y')
                    one_year_ago = datetime.now() - timedelta(days=365)
                    
                    is_valid = report_date >= one_year_ago
                    return is_valid, date_str, f"Rapor tarihi: {date_str} {'(GE√áERLƒ∞)' if is_valid else '(GE√áERSƒ∞Z - 1 yƒ±ldan eski)'}"
                except ValueError:
                    continue
        
        return False, "", "Rapor tarihi bulunamadƒ±"
    
    def analyze_criteria(self, text: str, category: str) -> Dict[str, ESPEAnalysisResult]:
        """Belirli kategori kriterlerini analiz etme"""
        results = {}
        criteria = self.criteria_details.get(category, {})
        
        # Dil tespiti yap
        detected_lang = self.detect_language(text)
        logger.info(f"Tespit edilen dil: {detected_lang}")
        
        for criterion_name, criterion_data in criteria.items():
            pattern = criterion_data["pattern"]
            weight = criterion_data["weight"]
            
            # Ana pattern ile ara
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            
            # Eƒüer bulunamadƒ±ysa, dile √∂zel ek pattern'ler dene
            if not matches:
                additional_patterns = self.get_multilingual_patterns(criterion_name, detected_lang)
                for add_pattern in additional_patterns:
                    matches = re.findall(add_pattern, text, re.IGNORECASE | re.MULTILINE)
                    if matches:
                        break
            
            # √ñzel durum: Durma zamanlarƒ± i√ßin tablo formatƒ± arama
            if not matches and criterion_name in ['durus_suresi_min', 'durus_suresi_max']:
                # Tablo formatƒ±nda sayƒ± arama - Min/Max ile birlikte
                table_patterns = [
                    r"Min\s*(\d{2,3})",
                    r"Maks?\s*(\d{2,3})", 
                    r"(\d{2,3})\s*(?=.*(?:Min|Maks?))",
                    r"(\d{2,3})\s*(?=.*\d{2,3}.*(?:Min|Maks?))",
                    r"(?:Durma\s*Zamanƒ±|STT).*?(\d{2,3})"
                ]
                
                for table_pattern in table_patterns:
                    table_matches = re.findall(table_pattern, text, re.IGNORECASE | re.MULTILINE)
                    if table_matches:
                        matches = table_matches
                        break
            
            # √ñzel durum: √ñl√ß√ºm metodu i√ßin geni≈ü arama
            if not matches and criterion_name == 'olcum_metodu':
                method_patterns = [
                    r"(?:Yapƒ±lan.*?√ñl√ß√ºm|ESPE.*?√ñl√ß√ºm|Test.*?Prosed√ºr)",
                    r"(?:Durma\s*Zamanƒ±|STT|STD)",
                    r"(?:Tehlikeli\s*Hareket|Mevcut\s*Emniyet)"
                ]
                
                for method_pattern in method_patterns:
                    method_matches = re.findall(method_pattern, text, re.IGNORECASE | re.MULTILINE)
                    if method_matches:
                        matches = method_matches
                        break
            
            if matches:
                content = str(matches[0]) if len(matches) == 1 else str(matches)
                found = True
                score = weight
            else:
                # Genel fallback pattern'ler
                general_patterns = {
                    "proje_adi_numarasi": r"[A-Z]\d{2}\.\d{3}",
                    "makine_adi": r"(?:T\d+|MCC\d+)",
                    "cihaz_tipi": r"(?:Light|I≈üƒ±k|Licht|ESPE)",
                    "marka_model": r"(?:DataLogic|SAFEasy|Pilz|Sick|Banner|Omron|Keyence)",
                    "formula_s": r"S\s*=",
                    "tehlike_tanimi": r"(?:Dangerous|Tehlike|Gefahr|fikst√ºr|kapƒ±|hareket)",
                    "k_sabiti": r"(?:2000|1600)",
                    "uygunluk_kontrolu": r"(?:UYGUN|SUITABLE|UYGUNSUZ|UNSUITABLE)",
                    "olcum_metodu": r"(?:ESPE|√ñl√ß√ºm|Test|Measurement)",
                    "durus_suresi_min": r"(?:Min|\d{2,3}(?=.*\d{2,3}))",
                    "durus_suresi_max": r"(?:Maks?|\d{2,3}(?=.*Min))",
                    "test_sayisi": r"(?:Test|Tekrar|\d+)"
                }
                
                general_pattern = general_patterns.get(criterion_name)
                if general_pattern:
                    general_matches = re.findall(general_pattern, text, re.IGNORECASE)
                    if general_matches:
                        content = f"Genel e≈üle≈üme bulundu: {general_matches[0]}"
                        found = True
                        score = weight // 2  # Kƒ±smi puan
                    else:
                        content = "Bulunamadƒ±"
                        found = False
                        score = 0
                else:
                    content = "Bulunamadƒ±"
                    found = False
                    score = 0
            
            results[criterion_name] = ESPEAnalysisResult(
                criteria_name=criterion_name,
                found=found,
                content=content,
                score=score,
                max_score=weight,
                details={"pattern_used": pattern, "matches_found": len(matches) if matches else 0}
            )
        
        return results
    
    def extract_specific_values(self, text: str) -> Dict[str, Any]:
        """Spesifik deƒüerleri √ßƒ±karma"""
        values = {}
        
        # √áoklu dil deƒüer pattern'leri - basit ve genel
        value_patterns = {
            "proje_no": r"(C\d{2}\.\d{3})",
            "olcum_tarihi": r"(\d{1,2}[./]\d{1,2}[./]\d{4})",
            "makine_adi": r"(?:Makine\s*Ad[ƒ±i][:]*\s*(T\d+\s*-\s*MCC\d+))",
            "hat_bolge": r"(?:Jaws\s*\d+|Hat.*?[:=]\s*([^\n\r]+))",
            "koruma_yuksekligi": r"(\d{3,4})\s*mm",
            "cozunurluk": r"(\d{1,2})\s*mm",
            "durus_suresi_min": r"Min\.?\s*(\d{2,3})|Minimum\s*(\d{2,3})",
            "durus_suresi_max": r"Maks?\.?\s*(\d{2,3})|Max\.?\s*(\d{2,3})|Maximum\s*(\d{2,3})|(\d{2,3})\s*(?=.*Maks?)|(\d{2,3})\s*(?=.*Max)",
            "mevcut_mesafe": r"(\d{2,4})\s*mm",
            "hesaplanan_mesafe": r"(?:S\s*=\s*(\d{2,4})|(\d{2,4})\s*mm)",
            "durum": r"(UYGUNSUZ|UYGUN)",
            "tehlikeli_hareket": r"(?:Takƒ±m\s*tezgahƒ±|kapƒ±\s*kapanma|fikst√ºr|tehlikeli\s*hareket)",
            "k_sabiti": r"(?:K\s*=\s*(\d{4})|2000|1600)",
            "formula_s": r"(S\s*=\s*\([^)]+\))",
            "formula_c": r"(C\s*=\s*8\s*[√óx*]\s*\([^)]+\))",
            "en_iso_13855": r"(EN\s*ISO\s*13855)"
        }
        
        # Dil tespiti
        detected_lang = self.detect_language(text)
        
        for key, pattern in value_patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                # √ñzel durum: durum i√ßin UYGUNSUZ varsa onu tercih et
                if key == "durum" and len(matches) > 1:
                    uygunsuz_found = any("UYGUNSUZ" in str(m).upper() for m in matches)
                    if uygunsuz_found:
                        values[key] = "UYGUNSUZ"
                    else:
                        # ƒ∞lk grubu al (eƒüer gruplar varsa)
                        if isinstance(matches[0], tuple):
                            values[key] = next((m for m in matches[0] if m), matches[0][0]).strip()
                        else:
                            values[key] = matches[0].strip()
                else:
                    # ƒ∞lk grubu al (eƒüer gruplar varsa)
                    if isinstance(matches[0], tuple):
                        values[key] = next((m for m in matches[0] if m), matches[0][0]).strip()
                    else:
                        values[key] = matches[0].strip()
            else:
                # Fallback: Basit pattern'ler
                fallback_patterns = {
                    "proje_no": r"C\d{2}\.\d{3}",
                    "olcum_tarihi": r"\d{1,2}[./]\d{1,2}[./]\d{4}",
                    "cihaz_tipi": r"(?:I≈üƒ±k\s*Perdesi|Light\s*Curtain|Lichtvorhang)",
                    "durum": r"(?:durumu[:]*\s*(UYGUNSUZ|UYGUN)|(?:UYGUNSUZ|UYGUN)(?=\s|$))",
                    "makine_adi": r"(?:T\d+\s*-\s*MCC\d+)",
                    "hat_bolge": r"(?:Jaws|\d+\.?\s*Hat)",
                    "koruma_yuksekligi": r"\d{3,4}\s*mm",
                    "cozunurluk": r"\d{1,2}\s*mm",
                    "durus_suresi_min": r"(?:Min|(\d{2,3})(?=.*\d{2,3}))",
                    "durus_suresi_max": r"(?:Maks?|(\d{2,3})(?=.*Min))",
                    "olcum_metodu": r"(?:ESPE|√ñl√ß√ºm|Test|Measurement)"
                }
                
                fallback_pattern = fallback_patterns.get(key)
                if fallback_pattern:
                    fallback_matches = re.findall(fallback_pattern, text, re.IGNORECASE)
                    if fallback_matches:
                        values[key] = fallback_matches[0].strip()
                    else:
                        values[key] = "Bulunamadƒ±"
                else:
                    values[key] = "Bulunamadƒ±"
        
        return values
    
    def validate_extracted_values(self, extracted_values: Dict[str, Any]) -> Dict[str, float]:
        """√áƒ±karƒ±lan deƒüerlerin ge√ßerliliƒüini kontrol ederek puan azaltma fakt√∂r√º hesapla"""
        validation_scores = {}
        
        # Kritik deƒüerlerin kontrolleri
        validations = {
            # Bo≈ü veya "Bulunamadƒ±" deƒüerler
            "durus_suresi_min": 0.0 if not extracted_values.get("durus_suresi_min") or extracted_values.get("durus_suresi_min") == "Bulunamadƒ±" else 1.0,
            "durus_suresi_max": 0.0 if not extracted_values.get("durus_suresi_max") or extracted_values.get("durus_suresi_max") == "Bulunamadƒ±" else 1.0,
            
            # Makine adƒ± kontrol (T ile ba≈ülamalƒ±)
            "makine_adi": 1.0 if extracted_values.get("makine_adi", "").startswith("T") else 0.5,
            
            # UYGUNSUZ durumu tespit edilmeli
            "durum": 0.5 if extracted_values.get("durum", "").upper() in ["UYGUN", "SUITABLE"] else 1.0,
            
            # Sayƒ±sal deƒüerlerin mantƒ±klƒ± olmasƒ±
            "koruma_yuksekligi": 1.0 if extracted_values.get("koruma_yuksekligi", "0").isdigit() and int(extracted_values.get("koruma_yuksekligi", "0")) > 100 else 0.5,
            "cozunurluk": 1.0 if extracted_values.get("cozunurluk", "0").isdigit() and int(extracted_values.get("cozunurluk", "0")) > 5 else 0.5,
        }
        
        return validations

    def calculate_scores(self, analysis_results: Dict[str, Dict[str, ESPEAnalysisResult]], extracted_values: Dict[str, Any]) -> Dict[str, Any]:
        """Puanlarƒ± hesaplama - √ßƒ±karƒ±lan deƒüerlerin ge√ßerliliƒüini de kontrol ederek"""
        category_scores = {}
        total_score = 0
        total_max_score = 100
        
        # Deƒüer ge√ßerlilik kontrol√º
        validation_scores = self.validate_extracted_values(extracted_values)
        
        for category, results in analysis_results.items():
            category_max = self.criteria_weights[category]
            category_earned = 0
            category_possible = sum(result.max_score for result in results.values())
            
            # Her kriter i√ßin puanƒ± hesapla
            for criterion_name, result in results.items():
                base_score = result.score
                
                # Eƒüer bu kriter i√ßin ge√ßerlilik kontrol√º varsa uygula
                if criterion_name in validation_scores:
                    validation_factor = validation_scores[criterion_name]
                    adjusted_score = base_score * validation_factor
                    category_earned += adjusted_score
                else:
                    category_earned += base_score
            
            # Kategori puanƒ±nƒ± aƒüƒ±rlƒ±ƒüa g√∂re normalize et
            normalized_score = (category_earned / category_possible * category_max) if category_possible > 0 else 0
            
            category_scores[category] = {
                "earned": category_earned,
                "possible": category_possible,
                "normalized": round(normalized_score, 2),
                "max_weight": category_max,
                "percentage": round((category_earned / category_possible * 100), 2) if category_possible > 0 else 0
            }
            
            total_score += normalized_score
        
        return {
            "category_scores": category_scores,
            "total_score": round(total_score, 2),
            "total_max_score": total_max_score,
            "overall_percentage": round((total_score / total_max_score * 100), 2)
        }
    
    def generate_detailed_report(self, pdf_path: str, docx_path: str = None) -> Dict[str, Any]:
        """Detaylƒ± rapor olu≈üturma"""
        logger.info("ESPE rapor analizi ba≈ülatƒ±lƒ±yor...")
        
        # PDF'den metin √ßƒ±kar
        pdf_text = self.extract_text_from_pdf(pdf_path)
        if not pdf_text:
            return {"error": "PDF okunamadƒ±"}
        
        # Dil tespiti
        detected_language = self.detect_language(pdf_text)
        logger.info(f"Tespit edilen belge dili: {detected_language}")
        
        # Tarih ge√ßerliliƒüi kontrol√º
        date_valid, date_str, date_message = self.check_report_date_validity(pdf_text)
        
        # Spesifik deƒüerleri √ßƒ±kar
        extracted_values = self.extract_specific_values(pdf_text)
        
        # Her kategori i√ßin analiz yap
        analysis_results = {}
        for category in self.criteria_weights.keys():
            analysis_results[category] = self.analyze_criteria(pdf_text, category)
        
        # Puanlarƒ± hesapla
        scores = self.calculate_scores(analysis_results, extracted_values)
        
        # √ñneriler olu≈ütur
        recommendations = self.generate_recommendations(analysis_results, scores)
        
        report = {
            "analiz_tarihi": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "dosya_bilgileri": {
                "pdf_path": pdf_path,
                "docx_path": docx_path,
                "tespit_edilen_dil": detected_language
            },
            "tarih_gecerliligi": {
                "gecerli": date_valid,
                "tarih": date_str,
                "mesaj": date_message
            },
            "cikarilan_degerler": extracted_values,
            "kategori_analizleri": analysis_results,
            "puanlama": scores,
            "oneriler": recommendations,
            "ozet": {
                "toplam_puan": scores["total_score"],
                "yuzde": scores["overall_percentage"],
                "durum": "GE√áERLƒ∞" if scores["overall_percentage"] >= 70 else "YETERSƒ∞Z",
                "tarih_durumu": "GE√áERLƒ∞" if date_valid else "GE√áERSƒ∞Z",
                "dil": detected_language
            }
        }
        
        return report
    
    def generate_recommendations(self, analysis_results: Dict, scores: Dict) -> List[str]:
        """√ñneriler olu≈üturma"""
        recommendations = []
        
        for category, results in analysis_results.items():
            category_score = scores["category_scores"][category]["percentage"]
            
            if category_score < 50:
                recommendations.append(f"‚ùå {category} b√∂l√ºm√º yetersiz (%{category_score:.1f})")
                
                # Eksik kriterler
                missing_criteria = [name for name, result in results.items() if not result.found]
                if missing_criteria:
                    recommendations.append(f"  Eksik kriterler: {', '.join(missing_criteria)}")
            
            elif category_score < 80:
                recommendations.append(f"‚ö†Ô∏è {category} b√∂l√ºm√º geli≈ütirilmeli (%{category_score:.1f})")
            
            else:
                recommendations.append(f"‚úÖ {category} b√∂l√ºm√º yeterli (%{category_score:.1f})")
        
        # Genel √∂neriler
        if scores["overall_percentage"] < 70:
            recommendations.append("\nüö® GENEL √ñNERƒ∞LER:")
            recommendations.append("- Rapor EN ISO 13855 standardƒ±na tam uyumlu hale getirilmelidir")
            recommendations.append("- Eksik bilgiler tamamlanmalƒ±dƒ±r")
            recommendations.append("- Form√ºl hesaplamalarƒ± detaylandƒ±rƒ±lmalƒ±dƒ±r")
        
        return recommendations
    
def main():
    """Ana fonksiyon"""
    analyzer = ESPEReportAnalyzer()
    
    # Dosya yollarƒ±
    pdf_path = "T4-MCC1201 - ESPE Kontrol Raporu.pdf"
    docx_path = "ESPE_Rapor_Kriterleri_Puanlama.docx"
    
    # Dosyalarƒ±n varlƒ±ƒüƒ±nƒ± kontrol et
    if not os.path.exists(pdf_path):
        print(f"‚ùå PDF dosyasƒ± bulunamadƒ±: {pdf_path}")
        return
    
    print("üîç ESPE Rapor Analizi Ba≈ülatƒ±lƒ±yor...")
    print("=" * 60)
    
    # Analizi √ßalƒ±≈ütƒ±r
    report = analyzer.generate_detailed_report(pdf_path, docx_path)
    
    if "error" in report:
        print(f"‚ùå Hata: {report['error']}")
        return
    
    # Sonu√ßlarƒ± g√∂ster
    print("\nüìä ANALƒ∞Z SONU√áLARI")
    print("=" * 60)
    
    print(f"üìÖ Analiz Tarihi: {report['analiz_tarihi']}")
    print(f"üåê Tespit Edilen Dil: {report['ozet']['dil'].title()}")
    print(f"üìã Toplam Puan: {report['ozet']['toplam_puan']}/100")
    print(f"üìà Y√ºzde: %{report['ozet']['yuzde']}")
    print(f"üéØ Durum: {report['ozet']['durum']}")
    print(f"üìÜ Tarih Durumu: {report['ozet']['tarih_durumu']}")
    
    print(f"\n‚ö†Ô∏è Tarih Kontrol√º: {report['tarih_gecerliligi']['mesaj']}")
    
    print("\nüìã √ñNEMLƒ∞ √áIKARILAN DEƒûERLER")
    print("-" * 40)
    important_values = ['proje_no', 'olcum_tarihi', 'makine_adi', 'marka', 'model', 
                       'koruma_yuksekligi', 'cozunurluk', 'durus_suresi_min', 'durus_suresi_max',
                       'mevcut_mesafe', 'hesaplanan_mesafe', 'durum', 'tehlikeli_hareket', 
                       'k_sabiti', 'formula_s', 'en_iso_13855']
    
    for key in important_values:
        if key in report['cikarilan_degerler']:
            print(f"{key.replace('_', ' ').title()}: {report['cikarilan_degerler'][key]}")
    
    print("\nüìä KATEGORƒ∞ PUANLARI")
    print("-" * 40)
    for category, score_data in report['puanlama']['category_scores'].items():
        print(f"{category}: {score_data['normalized']}/{score_data['max_weight']} (%{score_data['percentage']:.1f})")
    
    print("\nüí° √ñNERƒ∞LER")
    print("-" * 40)
    for recommendation in report['oneriler']:
        print(recommendation)

if __name__ == "__main__":
    main()